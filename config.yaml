
data:
  extract:
    enable: 0
    merge: ["tstrip", "sheeblimp"] # whatever name is first will be merged all into this
    ignore: ["isaacbarzso", "Maki", "Alistair", "Craig"]
    clean: False # delete zipped archives as we extract
    input_zip: raw-small.zip # relative to 'in' directory below
    dir:
      in: data/raw # relative to main workspace
      out: data/extracted

  split:
    enable: 0
    ignore: []
    max_splits: 100 # stop after we process more than 500 splits per speaker
    progress: True
    verbose: False
    accurate: False
    clean: False
    out_format: wav # sentences will be in this final format       
    sample_rate_hz: 22050
    num_channels: 1 # mono
    silence_db: -40
    min_silence_s: 2
    min_nonsilence_s: 2
    dir:
      in: data/extracted/raw-small/speaker
      out: data/processed

  transcribe:
    enable: 0
    model: turbo
    verbose: True
    min_confidence: -0.5 # avglogprob minimum, otherwise discard it
    del_wav_if_no_transcribe: True
    ignore: []
    dir:
      in: data/processed # local

load:
  enable: 1
  resample: False
  dummy_data: True

  audio:
    sr: 16000
    frame_size_s: 0.05
    frame_hop_s: 0.0125
    audio_length_s: 30.0
    n_mels: 80
    window: hann
    min_mag: 0.01

  dir:
    in: data/processed/1-scott
    out: data/processed/1-scott

tacotron2:
  enable: 0
  load_checkpoint: #"tacotron2_checkpoint.pth" 
  save_checkpoint: #"tacotron2_checkpoint.pth"
  tokenizer: WAVERNN_CHAR_LJSPEECH
  seed: 1337
  hyperparams:
    lr: 0.001
    betas: [0.9, 0.999]
    eps: 1e-08
    weight_decay: 1e-06
    epochs: 1
    split: [0.5, 0.25, 0.25] # train, val, test
    batch_size: 12 # with 12 we are pretty much at limit of GPU memory
    kernel_size: 5
  dir:
    out: data/processed/1-scott

wavernn:
  enable: 1
